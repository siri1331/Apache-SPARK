{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950e0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as tp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79143ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.sql.shuffle.partitions\", \"200\").config(\"spark.executor.memory\", \"4g\").config(\"master\", \"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa05eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa746851748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa459780",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv(\"dataset/ml_project/train.csv\",header = True, inferSchema = True)\n",
    "test = spark.read.csv(\"dataset/ml_project/test.csv\",header = True, inferSchema = True)\n",
    "valid = spark.read.csv(\"dataset/ml_project/valid.csv\",header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5dce7b",
   "metadata": {},
   "source": [
    "## Varibale Identification - numeric/categorical/temporal/boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2bebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Carrier: double (nullable = true)\n",
      " |-- TrafficType: string (nullable = true)\n",
      " |-- ClickDate: string (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Browser: string (nullable = true)\n",
      " |-- OS: string (nullable = true)\n",
      " |-- ConversionStatus: boolean (nullable = true)\n",
      " |-- publisherId: string (nullable = true)\n",
      " |-- advertiserCampaignId: double (nullable = true)\n",
      " |-- Fraud: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29b9a",
   "metadata": {},
   "source": [
    "* carrier is double but type of category\n",
    "* PublisherId is also categorical in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c90098",
   "metadata": {},
   "source": [
    "# Univariate Analysis balanced / imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac3bbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ConversionStatus</th><th>no of status</th><th>percentage</th></tr>\n",
       "<tr><td>null</td><td>0</td><td>0.0</td></tr>\n",
       "<tr><td>true</td><td>550</td><td>6.055801457796569E-4</td></tr>\n",
       "<tr><td>false</td><td>907669</td><td>0.9993933187994098</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+------------+--------------------+\n",
       "|ConversionStatus|no of status|          percentage|\n",
       "+----------------+------------+--------------------+\n",
       "|            null|           0|                 0.0|\n",
       "|            true|         550|6.055801457796569E-4|\n",
       "|           false|      907669|  0.9993933187994098|\n",
       "+----------------+------------+--------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for value_counts use groupby and count function\n",
    "train.groupBy(F.col(\"ConversionStatus\")).agg(F.count(\"ConversionStatus\").alias(\"no of status\"), (F.count(\"ConversionStatus\")/train.count()).alias(\"percentage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c6599",
   "metadata": {},
   "source": [
    "### Type casting target variable in test, train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f06e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"ConversionStatus\",F.col(\"ConversionStatus\").cast(tp.IntegerType()))\n",
    "test = test.withColumn(\"ConversionStatus\",F.col(\"ConversionStatus\").cast(tp.IntegerType()))\n",
    "valid = valid.withColumn(\"ConversionStatus\",F.col(\"ConversionStatus\").cast(tp.IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67894f7b",
   "metadata": {},
   "source": [
    "### Count the number of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07d77b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column : ID  Nulls count:0\n",
      "Column : Country  Nulls count:4889\n",
      "Column : Carrier  Nulls count:0\n",
      "Column : TrafficType  Nulls count:130051\n",
      "Column : ClickDate  Nulls count:0\n",
      "Column : Device  Nulls count:1197\n",
      "Column : Browser  Nulls count:3256\n",
      "Column : OS  Nulls count:3087\n",
      "Column : ConversionStatus  Nulls count:1\n",
      "Column : publisherId  Nulls count:1\n",
      "Column : advertiserCampaignId  Nulls count:5\n",
      "Column : Fraud  Nulls count:1\n"
     ]
    }
   ],
   "source": [
    "for column in train.columns:\n",
    "    null_condition = F.isnull(column)\n",
    "    nulls_count = train.filter(null_condition).count()\n",
    "    print(f\"Column : {column}  Nulls count:{nulls_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3092f9e",
   "metadata": {},
   "source": [
    "### Distict values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b638a3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ID</th><th>Country</th><th>Carrier</th><th>TrafficType</th><th>ClickDate</th><th>Device</th><th>Browser</th><th>OS</th><th>ConversionStatus</th><th>publisherId</th><th>advertiserCampaignId</th><th>Fraud</th></tr>\n",
       "<tr><td>908220</td><td>189</td><td>284</td><td>2</td><td>28251</td><td>694</td><td>28</td><td>15</td><td>2</td><td>1841</td><td>485</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+-------+-------+-----------+---------+------+-------+---+----------------+-----------+--------------------+-----+\n",
       "|    ID|Country|Carrier|TrafficType|ClickDate|Device|Browser| OS|ConversionStatus|publisherId|advertiserCampaignId|Fraud|\n",
       "+------+-------+-------+-----------+---------+------+-------+---+----------------+-----------+--------------------+-----+\n",
       "|908220|    189|    284|          2|    28251|   694|     28| 15|               2|       1841|                 485|    2|\n",
       "+------+-------+-------+-----------+---------+------+-------+---+----------------+-----------+--------------------+-----+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.agg(*[F.countDistinct(F.col(column)).alias(column) for column in train.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d26265",
   "metadata": {},
   "source": [
    "### Hence TrafficType, publisherId are also cateorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = train.groupBy(\"Country\").agg(F.count(\"Country\").alias(\"count\")).orderBy(\"count\", ascending = False).limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e52a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_countries = country_list.select(\"Country\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2137a3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Country='IN'),\n",
       " Row(Country='TH'),\n",
       " Row(Country='ID'),\n",
       " Row(Country='BD'),\n",
       " Row(Country='MX'),\n",
       " Row(Country='BR'),\n",
       " Row(Country='RU'),\n",
       " Row(Country='NG'),\n",
       " Row(Country='MY'),\n",
       " Row(Country='US'),\n",
       " Row(Country='BO'),\n",
       " Row(Country='PH'),\n",
       " Row(Country='ZA'),\n",
       " Row(Country='VE'),\n",
       " Row(Country='GT'),\n",
       " Row(Country='DZ'),\n",
       " Row(Country='KR'),\n",
       " Row(Country='CO'),\n",
       " Row(Country='IQ'),\n",
       " Row(Country='AE')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5682f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_countries = [row.Country for row in top_20_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f79a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IN',\n",
       " 'TH',\n",
       " 'ID',\n",
       " 'BD',\n",
       " 'MX',\n",
       " 'BR',\n",
       " 'RU',\n",
       " 'NG',\n",
       " 'MY',\n",
       " 'US',\n",
       " 'BO',\n",
       " 'PH',\n",
       " 'ZA',\n",
       " 'VE',\n",
       " 'GT',\n",
       " 'DZ',\n",
       " 'KR',\n",
       " 'CO',\n",
       " 'IQ',\n",
       " 'AE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2c5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_countries(country):\n",
    "    if country in top_20_countries:\n",
    "        return country\n",
    "    else:\n",
    "        return \"Others\"\n",
    "udf_map_countries = F.udf(f = map_countries, returnType = tp.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fffd66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Country_code\", udf_map_countries(train[\"Country\"]))\n",
    "test = test.withColumn(\"Country_code\", udf_map_countries(test[\"Country\"]))\n",
    "valid = valid.withColumn(\"Country_code\", udf_map_countries(valid[\"Country\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6476d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_country = train.withColumn(\"Country_code\", udf_map_countries(train[\"Country\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d49b0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_country = train_country.groupBy(\"Country_code\").agg(F.count(\"Country_code\")).orderBy(\"Country_code\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf1bc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_code</th>\n",
       "      <th>count(Country_code)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZA</td>\n",
       "      <td>10410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VE</td>\n",
       "      <td>10049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>17020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TH</td>\n",
       "      <td>148669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RU</td>\n",
       "      <td>26780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country_code  count(Country_code)\n",
       "0           ZA                10410\n",
       "1           VE                10049\n",
       "2           US                17020\n",
       "3           TH               148669\n",
       "4           RU                26780"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_countries_df = train_country.toPandas()\n",
    "top_20_countries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1728ef23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 21 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plt.title(\"Country-Wise Ads distribution\")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.bar(top_20_countries_df[\"Country_code\"], top_20_countries_df[\"count(Country_code)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a0ee5",
   "metadata": {},
   "source": [
    "# Value_counts for TrafficType, Fraud, Device(convert to Generic-1 and Non-Generic-0)\n",
    "### Apply same transformations for test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27341525",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o336.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 66 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o336.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 66 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>TrafficType</th><th>count</th></tr>\n",
       "<tr><td>A</td><td>537466</td></tr>\n",
       "<tr><td>M</td><td>240703</td></tr>\n",
       "<tr><td>null</td><td>0</td></tr>\n",
       "</table>\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"TrafficType\").agg(F.count(\"TrafficType\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca387d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o366.getRowsToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 72 (getRowsToPython at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mmax_num_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             sock_info = self._jdf.getRowsToPython(\n\u001b[0;32m--> 507\u001b[0;31m                 max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o366.getRowsToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 72 (getRowsToPython at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:31) \tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:628) \tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37) \tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1518) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+-----+------+\n",
       "|Fraud| count|\n",
       "+-----+------+\n",
       "|  0.0|908213|\n",
       "|  1.0|     6|\n",
       "| null|     0|\n",
       "+-----+------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"Fraud\").agg(F.count(\"Fraud\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf8c581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Device</th><th>count</th></tr>\n",
       "<tr><td>Generic</td><td>451505</td></tr>\n",
       "<tr><td>Samsung</td><td>128333</td></tr>\n",
       "<tr><td>Lenovo</td><td>38217</td></tr>\n",
       "<tr><td>ZTE</td><td>26566</td></tr>\n",
       "<tr><td>Micromax</td><td>26401</td></tr>\n",
       "<tr><td>Alcatel</td><td>26254</td></tr>\n",
       "<tr><td>Symphony</td><td>12645</td></tr>\n",
       "<tr><td>Lava</td><td>12431</td></tr>\n",
       "<tr><td>Intex</td><td>12124</td></tr>\n",
       "<tr><td>LG</td><td>11874</td></tr>\n",
       "<tr><td>Sony</td><td>9298</td></tr>\n",
       "<tr><td>Huawei</td><td>8726</td></tr>\n",
       "<tr><td>Apple</td><td>7422</td></tr>\n",
       "<tr><td>HTC</td><td>7075</td></tr>\n",
       "<tr><td>Motorola</td><td>6834</td></tr>\n",
       "<tr><td>Asus</td><td>5939</td></tr>\n",
       "<tr><td>true</td><td>5369</td></tr>\n",
       "<tr><td>Oukitel</td><td>4839</td></tr>\n",
       "<tr><td>Tecno</td><td>4779</td></tr>\n",
       "<tr><td>Gionee</td><td>4415</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+------+\n",
       "|  Device| count|\n",
       "+--------+------+\n",
       "| Generic|451505|\n",
       "| Samsung|128333|\n",
       "|  Lenovo| 38217|\n",
       "|     ZTE| 26566|\n",
       "|Micromax| 26401|\n",
       "| Alcatel| 26254|\n",
       "|Symphony| 12645|\n",
       "|    Lava| 12431|\n",
       "|   Intex| 12124|\n",
       "|      LG| 11874|\n",
       "|    Sony|  9298|\n",
       "|  Huawei|  8726|\n",
       "|   Apple|  7422|\n",
       "|     HTC|  7075|\n",
       "|Motorola|  6834|\n",
       "|    Asus|  5939|\n",
       "|    true|  5369|\n",
       "| Oukitel|  4839|\n",
       "|   Tecno|  4779|\n",
       "|  Gionee|  4415|\n",
       "+--------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"Device\").agg(F.count(\"Device\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa6770a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_mapping(x):\n",
    "    if x in [\"Generic\"]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "udf_device_mapping = F.udf(f = device_mapping, returnType = tp.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017948a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"Device\", udf_device_mapping(train[\"Device\"]))\n",
    "test = test.withColumn(\"Device\", udf_device_mapping(test[\"Device\"]))\n",
    "valid = valid.withColumn(\"Device\", udf_device_mapping(valid[\"Device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "025d57af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Device</th><th>count</th></tr>\n",
       "<tr><td>1</td><td>456715</td></tr>\n",
       "<tr><td>0</td><td>451505</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+------+\n",
       "|Device| count|\n",
       "+------+------+\n",
       "|     1|456715|\n",
       "|     0|451505|\n",
       "+------+------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"Device\").agg(F.count(\"Device\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e573b2b",
   "metadata": {},
   "source": [
    "# Browser \n",
    "* chrome : convert android_webkit, chrome, 46.0.2490.76(chrome version) and chromium\n",
    "* Safari : phone, safari\n",
    "* firefox : firefox_mobile, firefox, firefox_desktop\n",
    "* others : rest of them\n",
    "### Apply same transformations for test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a0e4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Browser</th><th>count</th></tr>\n",
       "<tr><td>android_webkit</td><td>429344</td></tr>\n",
       "<tr><td>chrome</td><td>378920</td></tr>\n",
       "<tr><td>46.0.2490.76</td><td>59461</td></tr>\n",
       "<tr><td>uc</td><td>16991</td></tr>\n",
       "<tr><td>chromium</td><td>7437</td></tr>\n",
       "<tr><td>iphone</td><td>7142</td></tr>\n",
       "<tr><td>opera</td><td>2843</td></tr>\n",
       "<tr><td>safari</td><td>630</td></tr>\n",
       "<tr><td>firefox_desktop</td><td>607</td></tr>\n",
       "<tr><td>msie</td><td>444</td></tr>\n",
       "<tr><td>firefox_mobile</td><td>268</td></tr>\n",
       "<tr><td>iemobile</td><td>261</td></tr>\n",
       "<tr><td>samsung</td><td>151</td></tr>\n",
       "<tr><td>tizen_browser</td><td>151</td></tr>\n",
       "<tr><td>blackberry</td><td>117</td></tr>\n",
       "<tr><td>ovi</td><td>83</td></tr>\n",
       "<tr><td>dorado</td><td>48</td></tr>\n",
       "<tr><td>fennec</td><td>26</td></tr>\n",
       "<tr><td>wap_browser</td><td>12</td></tr>\n",
       "<tr><td>netfront</td><td>7</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------------+------+\n",
       "|        Browser| count|\n",
       "+---------------+------+\n",
       "| android_webkit|429344|\n",
       "|         chrome|378920|\n",
       "|   46.0.2490.76| 59461|\n",
       "|             uc| 16991|\n",
       "|       chromium|  7437|\n",
       "|         iphone|  7142|\n",
       "|          opera|  2843|\n",
       "|         safari|   630|\n",
       "|firefox_desktop|   607|\n",
       "|           msie|   444|\n",
       "| firefox_mobile|   268|\n",
       "|       iemobile|   261|\n",
       "|        samsung|   151|\n",
       "|  tizen_browser|   151|\n",
       "|     blackberry|   117|\n",
       "|            ovi|    83|\n",
       "|         dorado|    48|\n",
       "|         fennec|    26|\n",
       "|    wap_browser|    12|\n",
       "|       netfront|     7|\n",
       "+---------------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"Browser\").agg(F.count(\"Browser\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d094f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_mapping(x):\n",
    "    if x in [\"android_webkit\",\"chrome\",\"46.0.2490.76\",\"Chromium\"]:\n",
    "        return \"chrome\"\n",
    "    elif x in [\"iphone\",\"safari\"]:\n",
    "        return \"safari\"\n",
    "    elif x in [\"firefox_mobile\",\"firefox\",\"firefoc_desktop\"]:\n",
    "        return \"firefox\"\n",
    "    else:\n",
    "        return \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7a38ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_browser = F.udf(f = browser_mapping, returnType = tp.StringType())\n",
    "train = train.withColumn(\"Browser\",udf_browser(train[\"Browser\"]))\n",
    "test = test.withColumn(\"Browser\",udf_browser(test[\"Browser\"]))\n",
    "valid = valid.withColumn(\"Browser\",udf_browser(valid[\"Browser\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8203e510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Browser</th><th>count</th></tr>\n",
       "<tr><td>chrome</td><td>867725</td></tr>\n",
       "<tr><td>others</td><td>32451</td></tr>\n",
       "<tr><td>safari</td><td>7772</td></tr>\n",
       "<tr><td>firefox</td><td>272</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------+\n",
       "|Browser| count|\n",
       "+-------+------+\n",
       "| chrome|867725|\n",
       "| others| 32451|\n",
       "| safari|  7772|\n",
       "|firefox|   272|\n",
       "+-------+------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"Browser\").agg(F.count(\"Browser\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379b407",
   "metadata": {},
   "source": [
    "## OS - 15 categories\n",
    "* Android: Android\n",
    "* ios : Mac OS X , IOS\n",
    "* others : rest of categories\n",
    "### Apply same transformations for test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a69d915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def os_mapping(x):\n",
    "    if x in [\"Android\"]:\n",
    "            return \"Android\"\n",
    "    elif x in [\"Mac OS X\", \"IOS\"]:\n",
    "        return \"ios\"\n",
    "    else:\n",
    "        return \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a19dc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_os = F.udf(f = os_mapping, returnType = tp.StringType())\n",
    "train = train.withColumn(\"OS\", udf_os(train[\"OS\"]))\n",
    "test = test.withColumn(\"OS\", udf_os(test[\"OS\"]))\n",
    "valid = valid.withColumn(\"OS\", udf_os(valid[\"OS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "662c273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>OS</th><th>count</th></tr>\n",
       "<tr><td>Android</td><td>893847</td></tr>\n",
       "<tr><td>Others</td><td>14327</td></tr>\n",
       "<tr><td>ios</td><td>46</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------+\n",
       "|     OS| count|\n",
       "+-------+------+\n",
       "|Android|893847|\n",
       "| Others| 14327|\n",
       "|    ios|    46|\n",
       "+-------+------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupBy(\"OS\").agg(F.count(\"OS\").alias(\"count\")).orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8052ae5",
   "metadata": {},
   "source": [
    "## PublisherId 2000 distict categories so --> PublisherId/frequency\n",
    "### groupby publisherId and count on ConversionStatus as total_p_id\n",
    "### Join this dataframe with original train df\n",
    "### Make transformations on text and valid data aswell\n",
    "*** this is a numeric column which as fixed number of distict values resulting in this transfomations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e747a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pub = train.groupBy(\"PublisherId\").agg(F.count(\"ConversionStatus\").alias(\"pub-id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c988557",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(train_pub, on = \"PublisherId\")\n",
    "test = test.join(train_pub, on = \"PublisherId\")\n",
    "valid = valid.join(train_pub, on = \"PublisherId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d5f9a3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o630.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 95 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o630.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 95 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898) \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>PublisherId</th><th>pub-id</th></tr>\n",
       "<tr><td>18314</td><td>1</td></tr>\n",
       "<tr><td>31713</td><td>1</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>35640</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37311</td><td>7</td></tr>\n",
       "<tr><td>37774</td><td>1</td></tr>\n",
       "<tr><td>38223</td><td>6</td></tr>\n",
       "<tr><td>38223</td><td>6</td></tr>\n",
       "<tr><td>38223</td><td>6</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(\"PublisherId\",\"pub-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc237b3",
   "metadata": {},
   "source": [
    "# advertiserCampaignId follow similar steps as PiblisherID\n",
    "### groupby advertiserCampaignId count on ConversionStatus and join this df with original on advertiserCampaignId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f869a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pub = train.groupBy(\"advertiserCampaignId\").agg(F.count(\"ConversionStatus\").alias(\"advertise-id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a77ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(train_pub, on = \"advertiserCampaignId\")\n",
    "test = test.join(train_pub, on = \"advertiserCampaignId\")\n",
    "valid = valid.join(train_pub, on = \"advertiserCampaignId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca9eca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.select(\"advertiserCampaignId\",\"advertise-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "032b4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.write.csv(\"train_uni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1022e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.write.csv(\"test_uni.csv\")\n",
    "# valid.write.csv(\"valid_uni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c9c8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = train.groupBy(\"Country_code\").agg(F.count(\"Country_code\").alias(\"country_count\"),F.sum(\"ConversionStatus\").alias(\"number_of_clicks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c13b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.orderBy(\"Country_code\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49c08394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = _df.withColumn(\"Percentage_Clicks\", _df[\"number_of_clicks\"]/_df[\"country_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426d5a21",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o726.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 131 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 67 \tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$2(MapOutputTracker.scala:1013) \tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$2$adapted(MapOutputTracker.scala:1009) \tat scala.collection.Iterator.foreach(Iterator.scala:941) \tat scala.collection.Iterator.foreach$(Iterator.scala:941) \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1009) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:821) \tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:133) \tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63) \tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57) \tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:190) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o726.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 131 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 67 \tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$2(MapOutputTracker.scala:1013) \tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$2$adapted(MapOutputTracker.scala:1009) \tat scala.collection.Iterator.foreach(Iterator.scala:941) \tat scala.collection.Iterator.foreach$(Iterator.scala:941) \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1009) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:821) \tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:133) \tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63) \tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57) \tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:190) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o726.getRowsToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 141 (getRowsToPython at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 10 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 10 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mmax_num_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             sock_info = self._jdf.getRowsToPython(\n\u001b[0;32m--> 507\u001b[0;31m                 max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o726.getRowsToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 141 (getRowsToPython at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 10 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 10 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "train_df.select(\"Country_code\",\"Percentage_Clicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99457e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o746.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 150 (toPandas at <ipython-input-46-848e35e2214f>:1) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:267)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:155)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-848e35e2214f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentage_Clicks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o746.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 150 (toPandas at <ipython-input-46-848e35e2214f>:1) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70) \tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.findNextInnerJoinRows$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774) \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) \tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) \tat org.apache.spark.scheduler.Task.run(Task.scala:131) \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown message type: 9 \tat org.apache.spark.network.shuffle.protocol.BlockTransferMessage$Decoder.fromByteBuffer(BlockTransferMessage.java:71) \tat org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:80) \tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:180) \tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat org.spark_project.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat org.spark_project.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat org.spark_project.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat org.spark_project.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat org.spark_project.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat org.spark_project.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \tat java.lang.Thread.run(Thread.java:748)  \tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:208) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) \tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) \tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) \tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) \tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) \tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) \tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) \tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) \tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) \tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) \t... 1 more \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1768)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2442)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:267)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:155)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_train = train_df.orderBy(\"Percentage_Clicks\",ascending = False).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title(\"Percentage of Clicks per Country\")\n",
    "plt.bar(df_train[\"Country_code\"], df_train[\"Percentage_Clicks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b52af3",
   "metadata": {},
   "source": [
    "### Ru- Russia has highest click rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da102b",
   "metadata": {},
   "source": [
    "## Browser vs number of clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ae829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupBy(\"Browser\").agg(F.count(\"Browser\").alias(\"no_browsers\"), F.sum(\"ConversionStatus\").alias(\"num_clicks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3629a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupBy(\"Device\").agg(F.count(\"Device\").alias(\"num_devices\"), F.sum(\"ConversionStatus\").alias(\"num_clicks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ffbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupBy(\"OS\").agg(F.count(\"OS\").alias(\"num_OS\"), F.sum(\"ConversionStatus\").alias(\"num_clicks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873571f",
   "metadata": {},
   "source": [
    "### FILL NULL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c24fa",
   "metadata": {},
   "source": [
    "* Country_code with IN --> as India is more frequent\n",
    "* TrafficType - U- undefined\n",
    "* Device with 1\n",
    "* Browser - chrome\n",
    "* OS -- > Android\n",
    "* advertise-id - 0\n",
    "* pub-id - 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fd0c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna({\n",
    "    \"Country_code\" : \"IN\",\n",
    "    \"Fraud\": 0,\n",
    "    \"TrafficType\": \"U\",\n",
    "    \"Device\": 1,\n",
    "    \"Browser\": \"chrome\",\n",
    "    \"OS\": \"Android\",\n",
    "    \"advertise-id\": 0,\n",
    "    \"pub-id\" : 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ca8da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna({\n",
    "    \"Country_code\" : \"IN\",\n",
    "    \"Fraud\": 0,\n",
    "    \"TrafficType\": \"U\",\n",
    "    \"Device\": 1,\n",
    "    \"Browser\": \"chrome\",\n",
    "    \"OS\": \"Android\",\n",
    "    \"advertise-id\": 0,\n",
    "    \"pub-id\" : 0\n",
    "})\n",
    "valid = valid.fillna({\n",
    "    \"Country_code\" : \"IN\",\n",
    "    \"Fraud\": 0,\n",
    "    \"TrafficType\": \"U\",\n",
    "    \"Device\": 1,\n",
    "    \"Browser\": \"chrome\",\n",
    "    \"OS\": \"Android\",\n",
    "    \"advertise-id\": 0,\n",
    "    \"pub-id\" : 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d53d8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## StringIndexer for Country_code, OS, Browser and TrafficType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "SI_Country = StringIndexer(inputCol = \"Country_code\",outputCol= \"Country_le\",handleInvalid = \"skip\")\n",
    "SI_OS = StringIndexer(inputCol = \"OS\",outputCol= \"OS_le\",handleInvalid = \"skip\")\n",
    "SI_Browser = StringIndexer(inputCol = \"Browser\",outputCol= \"Browser_le\",handleInvalid = \"skip\")\n",
    "SI_TrafficType = StringIndexer(inputCol = \"TrafficType\",outputCol= \"TrafficType_le\",handleInvalid = \"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5432da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_Country_OBJ= SI_Country.fit(train)\n",
    "SI_OS_OBJ= SI_OS.fit(train)\n",
    "SI_Browser_OBJ= SI_Browser.fit(train)\n",
    "SI_TrafficType_OBJ= SI_TrafficType.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f830285",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = SI_Country_OBJ.transform(train)\n",
    "train_encoded = SI_OS_OBJ.transform(train_encoded)\n",
    "train_encoded = SI_Browser_OBJ.transform(train_encoded)\n",
    "train_encoded = SI_TrafficType_OBJ.transform(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0ce6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87aa6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train = OneHotEncoder(inputCols = [\"Country_le\",\n",
    "                                      \"Browser_le\",\n",
    "                                      \"OS_le\",\n",
    "                                      \"TrafficType_le\"],\n",
    "                         outputCols = [\"Country_ohe\",\n",
    "                                       \"Browser_ohe\",\n",
    "                                      \"OS_ohe\",\n",
    "                                      \"TrafficTyep_ohe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45fcf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE_OBJ = ohe_train.fit(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94fcee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_ohe = OHE_OBJ.transform(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "777a6747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Country_code</th><th>Country_ohe</th><th>Browser</th><th>Browser_ohe</th><th>OS</th><th>OS_ohe</th><th>TrafficType</th><th>TrafficTyep_ohe</th></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>others</td><td>(3,[1],[1.0])</td><td>Others</td><td>(2,[1],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>IN</td><td>(20,[0],[1.0])</td><td>chrome</td><td>(3,[0],[1.0])</td><td>Android</td><td>(2,[0],[1.0])</td><td>A</td><td>(2,[0],[1.0])</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+--------------+-------+-------------+-------+-------------+-----------+---------------+\n",
       "|Country_code|   Country_ohe|Browser|  Browser_ohe|     OS|       OS_ohe|TrafficType|TrafficTyep_ohe|\n",
       "+------------+--------------+-------+-------------+-------+-------------+-----------+---------------+\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "|          IN|(20,[0],[1.0])| chrome|(3,[0],[1.0])|Android|(2,[0],[1.0])|          A|  (2,[0],[1.0])|\n",
       "+------------+--------------+-------+-------------+-------+-------------+-----------+---------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded_ohe.select(\"Country_code\",\"Country_ohe\",\"Browser\",\"Browser_ohe\",\"OS\",\"OS_ohe\",\"TrafficType\",\"TrafficTyep_ohe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2a38527",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = SI_Country_OBJ.transform(test)\n",
    "test_encoded = SI_OS_OBJ.transform(test_encoded)\n",
    "test_encoded = SI_Browser_OBJ.transform(test_encoded)\n",
    "test_encoded = SI_TrafficType_OBJ.transform(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39da4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_encoded = SI_Country_OBJ.transform(valid)\n",
    "valid_encoded = SI_OS_OBJ.transform(valid_encoded)\n",
    "valid_encoded = SI_Browser_OBJ.transform(valid_encoded)\n",
    "valid_encoded = SI_TrafficType_OBJ.transform(valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe1076e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded_ohe = OHE_OBJ.transform(test_encoded)\n",
    "valid_encoded_ohe = OHE_OBJ.transform(valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7209b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6149fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"Fraud\",\"Device\",\"OS_ohe\",\"TrafficTyep_ohe\",\"Browser_ohe\",\"Country_ohe\",\"pub-id\",\"advertise-id\"], outputCol = \"feature_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45b108f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = assembler.transform(train_encoded_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f4a6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = assembler.transform(test_encoded_ohe)\n",
    "valid_vec = assembler.transform(valid_encoded_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25a5224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- advertiserCampaignId: double (nullable = true)\n",
      " |-- publisherId: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Carrier: double (nullable = true)\n",
      " |-- TrafficType: string (nullable = false)\n",
      " |-- ClickDate: string (nullable = true)\n",
      " |-- Device: integer (nullable = false)\n",
      " |-- Browser: string (nullable = false)\n",
      " |-- OS: string (nullable = false)\n",
      " |-- ConversionStatus: integer (nullable = true)\n",
      " |-- Fraud: double (nullable = false)\n",
      " |-- Country_code: string (nullable = false)\n",
      " |-- pub-id: long (nullable = false)\n",
      " |-- advertise-id: long (nullable = false)\n",
      " |-- Country_le: double (nullable = false)\n",
      " |-- OS_le: double (nullable = false)\n",
      " |-- Browser_le: double (nullable = false)\n",
      " |-- TrafficType_le: double (nullable = false)\n",
      " |-- Country_ohe: vector (nullable = true)\n",
      " |-- Browser_ohe: vector (nullable = true)\n",
      " |-- OS_ohe: vector (nullable = true)\n",
      " |-- TrafficTyep_ohe: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_vec.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7a864fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import classification\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ade7b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = classification.LogisticRegression(featuresCol=\"feature_vector\",labelCol=\"ConversionStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "574e4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = model_lr.fit(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4f25dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7373501096469809"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = \"ConversionStatus\", metricName = \"areaUnderROC\")\n",
    "evaluator.evaluate(model_lr.transform(train_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45bb231a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6879895886254253"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_lr.transform(valid_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acbc5116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447644032085812"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_lr.transform(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed284792",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = classification.DecisionTreeClassifier(featuresCol = \"feature_vector\", labelCol = \"ConversionStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4034b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = model_dt.fit(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7867b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_dt.transform(train_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c53d68ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_dt.transform(valid_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "214d2a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_dt.transform(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4de96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = classification.RandomForestClassifier(featuresCol = \"feature_vector\", labelCol = \"ConversionStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43655d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = model_rf.fit(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3e31e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_rf.transform(train_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c21bae12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_rf.transform(valid_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10dafcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model_rf.transform(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64ce5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
