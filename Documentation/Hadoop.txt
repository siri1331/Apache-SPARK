Yarn: 
Resource Manager

Hbase: 
Works with NoSQL database allowing Random and Real-time read/writes

Apache PIG- 
Mapreduce taks are written in HLL Latin.

Apache HIVE: 
Datawarehouse
HQL(Hive query language
Execute queries using mapreduce
Data exploration and analysis

What is DataWarehouse:
Data comes from CRM, ERP and SCM--> ETL-->Datawarehouse(No data inconsistency)

ZooKeeper: Distributed coordination system, Naming service
Acts as a jobs scheduling manager at cluster level
ZooKeeper is used by Yarn aswell for its research allocation properties.

KAFKA:
Handles realtime streaming data
Ingests streaming data from various resources ===> various applications
Used for Monitoring the Operational data.
General messaging system
Pull model - can retain info until user pulls for new data and robust to node failures

Apache Flume :
Collects streaming or batch data form various resources to Hadoop. Specifincally designed for Hadoop.
Push model: It will not retain any info and user can get overwhelmed with new data, flume agent failure can lead to loss of events

SQOOP: Used in importing data from rdbms to Hadoop can export data back to RDBMS

Data Sources: 
Data Ingestion: flume and kafka, Sqoop
Data Storage: HDFS, HBASE
Data Preprocessing : Apache Spark , Hadoop Mapreduce
Data Exploration: HIVE, PIG
=======================================================================
Property Files:
=======================================================================
cd /etc/hadoop/conf
hadoop-env.sh -->java_home and hadoop.heap.size=15g
hdfs-site.xml --> nameNode, dataNode, blockSize, replication
core-site.xml --> fs.defaultFS which specifies the default file system URI
yarn-env.sh - resource allocation policies, node manager settings, and resource manager addresses.
yarn-site.xml
mapred-site.xml

cd /opt/spark tab tab and select the spark-hadoop version
spark-defaults.conf --> spark.master
                        spark.executor.instances -->Specifies the number of executors
                        spark.executor.memory -->their memory
                        spark.executor.cores --> the number of cores per executor.
                        shuffle configurations
                        serialization settings
                        logging levels
spark-env.sh --> SPARK_HOME: Specifies the location where Spark is installed.
                 JAVA_HOME: Specifies the location of the Java Development Kit (JDK) required by Spark.
                 SPARK_WORKER_MEMORY: Sets the amount of memory to allocate for each Spark worker.
                 SPARK_WORKER_INSTANCES: Defines the number of worker instances to start on each node.