Yarn: 
Resource Manager

Hbase: 
Works with NoSQL database allowing Random and Real-time read/writes

Apache PIG- 
Mapreduce taks are written in HLL Latin.

Apache HIVE: 
Datawarehouse
HQL(Hive query language
Execute queries using mapreduce
Data exploration and analysis

What is DataWarehouse:
Data comes from CRM, ERP and SCM--> ETL-->Datawarehouse(No data inconsistency)

ZooKeeper: Distributed coordination system, Naming service
Acts as a jobs scheduling manager at cluster level
ZooKeeper is used by Yarn aswell for its research allocation properties.

KAFKA:
Handles realtime streaming data
Ingests streaming data from various resources ===> various applications
Used for Monitoring the Operational data.
General messaging system
Pull model - can retain info until user pulls for new data and robust to node failures

Apache Flume :
Collects streaming or batch data form various resources to Hadoop. Specifincally designed for Hadoop.
Push model: It will not retain any info and user can get overwhelmed with new data, flume agent failure can lead to loss of events

SQOOP: Used in importing data from rdbms to Hadoop can export data back to RDBMS

Data Sources: 
Data Ingestion: flume and kafka, Sqoop
Data Storage: HDFS, HBASE
Data Preprocessing : Apache Spark , Hadoop Mapreduce
Data Exploration: HIVE, PIG
=======================================================================
Property Files:
=======================================================================
cd /etc/hadoop/conf
hadoop-env.sh -->java_home and hadoop.heap.size=15g
hdfs-site.xml --> nameNode, dataNode, blockSize, replication
core-site.xml --> fs.defaultFS which specifies the default file system URI
yarn-env.sh - resource allocation policies, node manager settings, and resource manager addresses.
yarn-site.xml
mapred-site.xml

cd /opt/spark tab tab and select the spark-hadoop version
spark-defaults.conf --> spark.master
                        spark.executor.instances -->Specifies the number of executors
                        spark.executor.memory -->their memory
                        spark.executor.cores --> the number of cores per executor.
                        shuffle configurations
                        serialization settings
                        logging levels
spark-env.sh --> SPARK_HOME: Specifies the location where Spark is installed.
                 JAVA_HOME: Specifies the location of the Java Development Kit (JDK) required by Spark.
                 SPARK_WORKER_MEMORY: Sets the amount of memory to allocate for each Spark worker.
                 SPARK_WORKER_INSTANCES: Defines the number of worker instances to start on each node.


===========================================================================
Hadoop Commands
===========================================================================
hdfs dfs -ls /public/retail_db/orders
hdfs dfs -ls -R hdfs_path
ls -lhtr /data/retail_db/orders

Getting files Metadata
-------------------------------------------------------------
hdfs dfs -fsck /public/retail_db/orders -files -blocks -locations
            files--> details of file names
            blocks -->details of blocks associated with files
            locations--> details of worner nodes where blocks are located

hdfs dfs -cp hdfs_path1 hdfs_path2
hdfs dfs -put local_path hdfs_path1
hdfs dfs -get hdfs_path local_path

hdfs dfs -rm -R -skipTrash hdfs_path
            skipTrash will permanently delete the files rathan than to recycle

Gives properties of files
-------------------------------------------------------------

hdfs dfs -stat hdfs_path
        Gives last modified timeStamp
hdfs dfs -stat %o/r/b hdfs_path
        %o --> blockSize
        %u --> replicationFactor
        %b --> fileSize
        %F --> file or directory
        %g --> groupName of the owner
        %u --> userName of the file

hdfs dfs -du -s -h hdfs_path
            Summarize disk usage in human readable format

hdfs dfs -df -h
            Current capacity and usage of HDFS

Preview of data
-------------------------------------------------------------
Only on csv,json and txt files
hdfs dfs -tail hdfs_path
            Only works when file size is less than 1Kb

hdfs dfs -cat hdfs_path|more
            see first few lines


Change Permissions
-------------------------------------------------------------
hdfs dfs -chown -R u+w/g+w/o+w hdfs_path
            u --> owner
            g --> group
            o --> other
            r,w,x --> read,write,execute
            -w --> remove write permission from u,g,o

            rwx-r-x-r-x ==> 755
            hdfs dfs -chmod -R 755 hdfs_path

Overriding properties
-------------------------------------------------------------
hdfs dfs -D dfs.blocksize=64M -D dfs.replication=3 -put /data/retail_db/orders /user/ana012815/orders